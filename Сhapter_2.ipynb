{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка статической модели для русского языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ru_core_news_lg\n",
    "import spacy as sp\n",
    "from spacy.symbols import ORTH, LEMMA\n",
    "from IPython.display import Image\n",
    "\n",
    "nlp = sp.load('ru_core_news_lg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc --  Я пошла гулять в кино.\n",
      "[('Я', 'PRON'), ('пошла', 'VERB'), ('гулять', 'VERB'), ('в', 'ADP'), ('кино', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Я пошла гулять в кино.\")\n",
    "print(\"doc -- \",doc)\n",
    "print([(w.text, w.pos_) for w in doc])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Глава 2. Токенизация\n",
    "Самое первое действие любого NLP-приложения — разбор текста на токены, которые могут быть словами, числами или знаками препинания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Музыка', '-', 'это', 'искусство', ',', 'которое', 'дано', 'не', 'всем', '.']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Музыка - это искусство, которое дано не всем.\")\n",
    "print([w.text for w in doc])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Глава 2. Лемматизация\n",
    "*Лемма — это базовая, фактически словарная форма токена. Например, лемма токена flying — fly* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Яблочный  -  яблочный\n",
      "уксус  -  уксус\n",
      "подходит  -  подходить\n",
      "для  -  для\n",
      "выпечки  -  выпечка\n",
      "тортов  -  торт\n",
      ".  -  .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Яблочный уксус подходит для выпечки тортов.\") \n",
    "for token in doc:\n",
    "    print(token.text, \" - \", token.lemma_) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Столбец слева содержит токены, столбец справа — соответствующие леммы."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Глава 2. Использование лемматизации для распознавания смысла__\n",
    "Нужно сделать так чтобы модель понимала слова использованные в узкиз кругах\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Я', 'хочу', 'поехать', 'в', 'Новочек'] \n",
      "\n",
      "--> ['token:Я lemma:я', 'token:хочу lemma:хотеть', 'token:поехать lemma:поехать', 'token:в lemma:в', 'token:Новочек lemma:новочек'] \n",
      "\n",
      "['я', 'хотеть', 'поехать', 'в', 'новочек']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Я хочу поехать в Новочек\")\n",
    "print([w.text for w in doc], \"\\n\")\n",
    "\n",
    "nlp.get_pipe(\"attribute_ruler\").add([[{\"TEXT\": \"Новочек\"}]], {\"LEMMA\": \"Новочеркасск\"})\n",
    "   \n",
    "print(\"-->\", ['token:%s lemma:%s' % (t.text, t.lemma_) for t in doc], \"\\n\")\n",
    "\n",
    "# проход по леммам, где Новочек заменился на Новочеркасск\n",
    "print([w.lemma_ for w in doc])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Глава 2. Частеричная разметка__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я  -  nsubj\n",
      "хочу  -  ROOT\n",
      "уйти  -  xcomp\n",
      "спать  -  xcomp\n",
      "пораньше  -  advmod\n",
      ".  -  punct\n",
      "[None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Я хочу уйти спать пораньше.\")\n",
    "print([print(token.text, \" - \",token.dep_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Часть речи        Токен        Лемма\n",
      "________________________________________\n",
      "        PRON            Я            я\n",
      "        VERB    прилетела    прилететь\n",
      "         ADP            в            в\n",
      "       PROPN        Питер        питер\n",
      "       PUNCT            .            .\n",
      "         ADV       Теперь       теперь\n",
      "        VERB       улетаю      улетать\n",
      "         ADP            в            в\n",
      "       PROPN       Москоу       москоу\n",
      "       PUNCT            .            .\n",
      "________________________________________\n",
      "['Я', 'прилетела', 'улетаю']\n",
      "________________________________________\n",
      "['Питер', 'Москоу']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Я прилетела в Питер. Теперь улетаю в Москоу.\")\n",
    "print('%12s %12s %12s' % (\"Часть речи\", \"Токен\", \"Лемма\")) \n",
    "print(\"_\" * 40)\n",
    "\n",
    "for token in doc:\n",
    "    print('%12s %12s %12s' % (token.tag_, token, token.lemma_)) \n",
    "\n",
    "print(\"_\" * 40)\n",
    "print([w.text for w in doc if w.tag_== 'VERB' or w.tag_== 'PRON'])\n",
    "\n",
    "print(\"_\" * 40)\n",
    "print([w.text for w in doc if w.pos_ == 'PROPN'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Синтаксические отношения. Главный и дочерние элементы.\n",
    "__Главный__ элемент отношения можно определить с помощью атрибута token.head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/picture_1.png\" width=\"500\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"images/picture_1.png\", width=500, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Я         PRON        nsubj\n",
      "   прилетела         VERB         ROOT\n",
      "           в          ADP         case\n",
      "       Питер        PROPN          obl\n",
      "           .        PUNCT        punct\n",
      "      Теперь          ADV       advmod\n",
      "      улетаю         VERB         ROOT\n",
      "           в          ADP         case\n",
      "      Москоу        PROPN          obl\n",
      "           .        PUNCT        punct\n",
      "________________________________________\n",
      "   прилетела        nsubj            Я\n",
      "   прилетела         ROOT    прилетела\n",
      "       Питер         case            в\n",
      "   прилетела          obl        Питер\n",
      "   прилетела        punct            .\n",
      "      улетаю       advmod       Теперь\n",
      "      улетаю         ROOT       улетаю\n",
      "      Москоу         case            в\n",
      "      улетаю          obl       Москоу\n",
      "      улетаю        punct            .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Я прилетела в Питер. Теперь улетаю в Москоу.\")\n",
    "for token in doc:\n",
    "    print('%12s %12s %12s' % (token.text, token.pos_, token.dep_))\n",
    "print(\"_\" * 40)\n",
    "for token in doc:\n",
    "    print('%12s %12s %12s' % (token.head.text, token.dep_, token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['прилетела', 'Питер']\n",
      "['улетаю', 'Москоу']\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print([w.text for w in sent if w.dep_ == 'ROOT' or w.dep_ == 'obl'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Распознавание именованных сущностей\n",
    "Именованная сущность (named entity) — объект, на который можно ссылаться по его собственному наименованию.\n",
    "ent_type - именнованная сущность\n",
    "\n",
    "LOC - локация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Питер          LOC\n",
      "      Москоу          LOC\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Я прилетела в Питер. Теперь улетаю в Москоу.\")\n",
    "for token in doc:\n",
    "    if token.ent_type != 0:\n",
    "        print('%12s %12s' % (token.text, token.ent_type_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
