{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ru_core_news_lg\n",
    "import spacy as sp\n",
    "from spacy.symbols import ORTH, LEMMA\n",
    "from IPython.display import Image\n",
    "from spacy.tokens.doc import Doc\n",
    "from spacy.vocab import Vocab\n",
    "from spacy import displacy\n",
    "\n",
    "#from spacy.lang.ru import Russian\n",
    "#nlp = Russian()\n",
    "\n",
    "nlp = sp.load('ru_core_news_lg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объекты-контейнеры библиотеки spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Привет всем "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Получение индекса токена в объекте Doc\n",
    "doc = nlp(\"Я хочу миндальную конфету\")\n",
    "[doc[i] for i in range(len(doc))]\n",
    "\n",
    "# Здесь вызываем конструктор класса Doc и передаем ему два параметра:\n",
    "# 1) объект vocab()— контейнер хранилища со словарными данными, например типами лексем (прилагательное, глагол, существительное и т. д.)\n",
    "# 2) и список токенов (words) для добавления в создаваемый объект Doc \n",
    "doc = Doc(Vocab(), words=[u'Привет', u'всем'])\n",
    "doc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обход в цикле синтаксических дочерних элементов токена"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[эту]\n",
      "[миндальную]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[миндальную]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Для получения программным образом левосторонних дочерних элементов токена \"конфета\"\n",
    "doc = nlp(\"Я хочу эту миндальную конфету\")\n",
    "print([w for w in doc[3].lefts])\n",
    "\n",
    "doc = nlp(\"Я хочу миндальную конфету\")\n",
    "print([w for w in doc[3].lefts])\n",
    "\n",
    "# Вывод -- работет странно?\n",
    "\n",
    "# Token.rights  -- > для получения право- сторонних синтаксических дочерних элементов\n",
    "# Token.children -- > для получения всех дочерних элементов\n",
    "[w for w in doc[3].children]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Контейнер doc.sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В небе запахло грозой.\n",
      "Скоро будет дождь.\n",
      "Пойдем книжку читать!\n",
      "           В          ADP         case\n",
      "        небе         NOUN          obl\n",
      "     запахло         VERB         ROOT\n",
      "      грозой         NOUN          obl\n",
      "           .        PUNCT        punct\n",
      "       Скоро          ADV       advmod\n",
      "       будет         VERB         ROOT\n",
      "       дождь         NOUN        nsubj\n",
      "           .        PUNCT        punct\n",
      "      Пойдем         VERB         ROOT\n",
      "      книжку         NOUN          obj\n",
      "      читать         VERB        xcomp\n",
      "           !        PUNCT        punct\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# С помощью свойства doc.sents объекта Doc текст можно разделить на отдельные предложения\n",
    "doc = nlp(u'В небе запахло грозой. Скоро будет дождь. Пойдем книжку читать!')\n",
    "for sent in doc.sents:\n",
    "    [sent[i] for i in range(len(sent))]\n",
    "    print(sent)\n",
    "\n",
    "# вспомним метод .pos_\n",
    "for token in doc:\n",
    "    print('%12s %12s %12s' % (token.text, token.pos_, token.dep_))\n",
    "\n",
    "\n",
    "# количество предложений оканчивающихся галаголом\n",
    "counter = 0\n",
    "for sent in doc.sents:\n",
    "    if sent[len(sent)-2].pos_ == 'VERB':\n",
    "        counter+=1\n",
    "print(counter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Контейнер doc.noun_chunks\n",
    "Именной фрагмент (noun chunk) — это фраза, главным элементом которой является существительное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "небе\n",
      "----------------------------------------\n",
      "небе\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'В небе запахло грозой. Скоро будет дождь. Пойдем книжку читать! Мир чудесен.')\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_=='NOUN':\n",
    "        chunk = ''\n",
    "        for w in token.children:\n",
    "            if w.pos_ == 'DET' or w.pos_ == 'ADJ':\n",
    "                chunk = chunk + w.text + ' '\n",
    "            chunk = chunk + token.text\n",
    "            print(chunk)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "for token in doc:\n",
    "    if token.pos_=='NOUN':\n",
    "        chunk = ''\n",
    "        for w in token.lefts:\n",
    "            chunk = chunk + token.text\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Объект Span\n",
    "Объект Span (от англ. span — «интервал») представляет собой часть объекта Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Мост         NOUN         ROOT         Мост\n",
      "     Золотые          ADJ         amod         Мост\n",
      "      Ворота        PROPN        appos         Мост\n",
      "           -        PUNCT         amod достопримечательность\n",
      "   культовая          ADJ         amod достопримечательность\n",
      "достопримечательность         NOUN        appos         Мост\n",
      "         Сан        PROPN         nmod достопримечательность\n",
      "           -        PROPN         nmod достопримечательность\n",
      "   Франциско        PROPN         nmod достопримечательность\n",
      "----------------------------------------\n",
      "Мост Золотые Ворота         NOUN         ROOT Мост Золотые Ворота\n",
      "           -        PUNCT         amod достопримечательность\n",
      "   культовая          ADJ         amod достопримечательность\n",
      "достопримечательность         NOUN        appos Мост Золотые Ворота\n",
      "Сан-Франциско        PROPN         nmod достопримечательность\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'В небе запахло грозой.')\n",
    "#print(doc[2:])\n",
    "\n",
    "# Span включает несколько методов, самый интересный из которых — span.retokenize().\n",
    "# С его помощью интервал можно объединять в единый токен, производя повторную токенизацию документа. \n",
    "# Это удобно, когда текст содержит названия из нескольких слов.\n",
    "\n",
    "doc = nlp(u'Мост Золотые Ворота - культовая достопримечательность Сан-Франциско')\n",
    "for token in doc:\n",
    "    print('%12s %12s %12s %12s' % (token.text, token.pos_, token.dep_,token.head.text))\n",
    "\n",
    "span = doc[0:3]\n",
    "span2 = doc[6:9]\n",
    "\n",
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(span)\n",
    "    retokenizer.merge(span2)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "for token in doc:\n",
    "    print('%12s %12s %12s %12s' % (token.text, token.pos_, token.dep_,token.head.text))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Настройка конвейера обработки текста под свои нужды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "----------------------------------------\n",
      "           Я         PRON             \n",
      "        хочу         VERB             \n",
      "      кислое          ADJ             \n",
      "     зеленое          ADJ             \n",
      "      яблоко         NOUN             \n",
      "Название использованной модели: ru_core_news_lg\n",
      "Путь: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ru_core_news_lg\n"
     ]
    }
   ],
   "source": [
    "from spacy import util\n",
    "\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Отключение компонентов конвейера\n",
    "nlp = sp.load('ru_core_news_lg', disable=['parser'])\n",
    "\n",
    "# В данном случае мы создадим конвейер обработки без утилиты разбора зависимостей. \n",
    "doc = nlp(u'Я хочу кислое зеленое яблоко')\n",
    "print(\"-\" * 40)\n",
    "for token in doc:\n",
    "    print('%12s %12s %12s' % (token.text, token.pos_, token.dep_))\n",
    "\n",
    "# метки зависимостей выведены не были.\n",
    "\n",
    "# полное название использованной модели\n",
    "print(\"Название использованной модели:\", nlp.meta['lang'] + '_' + nlp.meta['name'])\n",
    "print(\"Путь:\", util.get_package_path('ru_core_news_lg'))\n",
    "nlp = sp.load('ru_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# какие компоненты поддерживаются в контексте данной модели и могут быть загружены в конвейер\n",
    "print(nlp.meta['pipeline'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настройка компонентов конвейера под свои нужды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Яблоко LOC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "doc = nlp(u'Замечательный день сходить в Яблоко за кольцом')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "# Метка ORG обозначает различные компании, государственные бюро и прочие учреждения. \n",
    "# Но нам нужно, чтобы средство распознавания сущностей классифицировало его как сущность типа КОМПАНИЯ.\n",
    "\n",
    "LABEL = 'COMPANY'\n",
    "\n",
    "# правая граница не входит !!!\n",
    "TRAIN_DATA = [\n",
    "    ('Замечательный день сходить в Яблоко за кольцом', {'entities': [(29, 34, 'COMPANY')]}),\n",
    "    ('Приготовила сегодня пирог с яблоком', {'entities': []}),\n",
    "    ('Потратила все деньги в Яблоке.', {'entities': [(23, 29, 'КОМПАНИЯ')]}),\n",
    "]\n",
    "\n",
    "ner = nlp.get_pipe('ner')\n",
    "ner.add_label(LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "meta_on = nlp.meta['pipeline'] #список компонентов конвейера, используемых с моделью\n",
    "print(meta_on)\n",
    "\n",
    "# отключаем компоненты \n",
    "for component in meta_on:\n",
    "    if component != 'ner':\n",
    "        nlp.disable_pipes(component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Замечательный день сходить в Яблоко за кольцом\" with entities \"[(29, 34, 'COMPANY')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from spacy.training.example import Example\n",
    "\n",
    "for i in range(25):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    for batch in sp.util.minibatch(TRAIN_DATA, size = 3):\n",
    "        for text, annotations in batch:\n",
    "            # create Example\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            # Update the model\n",
    "            nlp.update([example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Яблоке КОМПАНИЯ\n",
      "Яблоки КОМПАНИЯ\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Потратила все деньги в Яблоке')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "doc = nlp(u'Съела все Яблоки')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учтите: внесенные только что обновления будут утрачены сразу после закрытия текущего сеанса интерпретатора Python. Для решения этой проблемы в классе Pipe, родительском для класса EntityRecognizer и других классов компонентов конвейера, предусмотрен метод to_ disk(), предназначенный для сериализации конвейера на диск:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nner.to_disk('/usr/to/ner')\\n\\nimport spacy\\nfrom spacy.pipeline import EntityRecognizer\\nnlp = spacy.load('en', disable=['ner'])\\n\\nner = EntityRecognizer(nlp.vocab)\\nner.from_disk('/usr/to/ner')\\nnlp.add_pipe(ner)\\n\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "ner.to_disk('/usr/to/ner')\n",
    "\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRecognizer\n",
    "nlp = spacy.load('en', disable=['ner'])\n",
    "\n",
    "ner = EntityRecognizer(nlp.vocab)\n",
    "ner.from_disk('/usr/to/ner')\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы загрузили модель, отключив ее компонент ner по умолчанию. Затем создали новый экземпляр ner, после чего загрузили данные с диска. Добавили компонент ner в конвейер обработки"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
